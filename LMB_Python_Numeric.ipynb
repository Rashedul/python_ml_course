{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though mathematical operations can be performed in regular Python the NumPy module is often faster and more convenient. Most of this revolves around n-dimensional array objects which store regular arrays of data of a specified type (usually, but not limited to, numeric types). Such arrays can be used in a similar way to lists: they contain an ordered sequence of values and can be used in loops etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25]\n",
      "[ 1  4  9 16 25]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # Load the NumPy module, assign it the name \"np\" for convienence\n",
    "\n",
    "l = [1,4,9,16,25] # A list\n",
    "a = np.array(l)   # An array built from a list\n",
    "\n",
    "print(l)\n",
    "print(a)\n",
    "print(type(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, an important idea with NumPy arrays is that operations can be performed on the array as a whole, rather than looping though all the component elements. This means that a fast internal implementation can be used. Also, it results in syntax similar to matrix algebra, where each variable is an entire array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5]\n",
      "[ 3  6  9 12 15]\n",
      "[ 1  4  9 16 25]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,3,4,5])\n",
    "print(a)\n",
    "print(3 * a)   #  All elements multiplied by 3\n",
    "print(a * a)   #  All elements squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An array will contain elements of the same data type. This data type is determined from the contents of the array when it is constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([2, 5, 1, 8, 0])\n",
    "z = np.array([3.14129, 2.71828, 1.41421])\n",
    "\n",
    "print(y.dtype)                   # int - whole numbers\n",
    "print(z.dtype)                   # float - fixed precision real numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data type of an array may be specifically stated (i.e. forced), irrespective of its initial elements. Data type may be converted (making a new array in the process) using `astype()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 5 1 8 0] int64\n",
      "[3.14129 2.71828 1.41421] [3 2 1]\n"
     ]
    }
   ],
   "source": [
    "y = np.array([2, 5, 1, 8, 0])  # Force float dtype\n",
    "x = z.astype(int)              # Convert to ints\n",
    "\n",
    "print(y, y.dtype)\n",
    "print(z, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many common operations are applied in an element-wise manner, i.e. to each value individually, and operations can work between two arrays if they have compatible sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 4. 5.]\n",
      "[1.5 2.  2.5]\n",
      "[4. 6. 8.]\n",
      "[ 3.  8. 15.]\n",
      "[-2. -2. -2.]\n",
      "[0.33333333 0.5        0.6       ]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1.0, 2.0, 3.0])\n",
    "y = np.array([3.0, 4.0, 5.0])\n",
    "\n",
    "print(x + 2)    # Add 2 to all values\n",
    "print(y / 2)    # Divide all values by 2\n",
    "\n",
    "# Element-wise operations bewtween different arrays with compatible sizes\n",
    "print(x + y)   \n",
    "print(x * y)   \n",
    "print(x - y)\n",
    "print(x / y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An array can have a number of different dimensions/axes, i.e. so that it can represent vectors, matrices tensors etc. Where appropriate, may operations are still applied in an element-wise manner even if an array has multiple axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[[0.01 0.02 0.03]\n",
      " [0.04 0.05 0.06]]\n",
      "(2, 3)\n",
      "6\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1,2,3],[4,5,6]])  # Make 2D array from list of lists\n",
    "print(x)\n",
    "print(x.shape)                   # (2,3) - rows x columns\n",
    "print(x.size)                    # 6 - elements in total\n",
    "print(x.ndim)                    # 2 - two axes\n",
    "print(x * 0.01)                  # Element-wise operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Array creation from differently sized sub-collections works, but may but gives a 1D array of Python objects, which may not be the expected result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([0, 1]) list([2, 3, 4]) list([5, 6, 7, 8])] object\n",
      "[list([0, 1, 0, 1]) list([2, 3, 4, 2, 3, 4])\n",
      " list([5, 6, 7, 8, 5, 6, 7, 8])]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0,1], [2,3,4], [5,6,7,8]])\n",
    "print(x, x.dtype)\n",
    "print(2 * x)       # Each list is multiplied by two, not the elements inside"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrays are easily reshaped to change the number of rows, columns axes etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1   2   3   4   5   6   7   8   9  10]\n",
      " [ 11  12  13  14  15  16  17  18  19  20]\n",
      " [ 21  22  23  24  25  26  27  28  29  30]\n",
      " [ 31  32  33  34  35  36  37  38  39  40]\n",
      " [ 41  42  43  44  45  46  47  48  49  50]\n",
      " [ 51  52  53  54  55  56  57  58  59  60]\n",
      " [ 61  62  63  64  65  66  67  68  69  70]\n",
      " [ 71  72  73  74  75  76  77  78  79  80]\n",
      " [ 81  82  83  84  85  86  87  88  89  90]\n",
      " [ 91  92  93  94  95  96  97  98  99 100]]\n",
      "(10, 10)\n",
      "100 100\n"
     ]
    }
   ],
   "source": [
    "x = np.arange(1,101)     # The NumPy equivalent of range()\n",
    "y = x.reshape(10,10)     # Convert to 10 rows x 10 columns\n",
    "print(y)\n",
    "print(y.shape)\n",
    "print(x.size, y.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various functions to create particular kinds of filled arrays, e.g. full of zeros or with linear increments, and the data type of the elements can generally be specified.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]]\n",
      "[[7. 7.]\n",
      " [7. 7.]]\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "[1.   1.25 1.5  1.75 2.   2.25 2.5  2.75]\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros((2,3))          # 2 x 3 array full of 0.0\n",
    "b = np.ones((3,2), int)      # 3 x 2 array full of 1\n",
    "c = np.full((2,2), 7.0)      # 2 x 2 array full of 7.0\n",
    "d = np.identity(3)           # 3 x 3 identity (1 on diagonal 0 elsewhere) \n",
    "e = np.arange(1.0, 3.0, 0.25) # From 1.0 up to <3.0 in steps of 0.25\n",
    "    \n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an index and range (slicing) syntax which is similar to that used with lists and strings etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  4  9 16 25 36 49 64 81]\n",
      "4\n",
      "[1 4 9]\n"
     ]
    }
   ],
   "source": [
    "x = np.arange(10) ** 2   # An array of sequential ints, squared\n",
    "print(x)\n",
    "print(x[2])         # number at index 2\n",
    "print(x[1:4])       # slice range (makes new array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ranges may also have a third argument to specify the increment (step) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4 16 36 64]\n",
      "[ 0  4 16 36 64]\n",
      "[81 64 49 36 25 16  9  4  1  0]\n"
     ]
    }
   ],
   "source": [
    "print(x[2:9:2])   # Start at index 2, increment 2\n",
    "print(x[::2])     # Every other element (start:end is implicit)\n",
    "print(x[::-1])    # Negative increments means backwards (end:start implicit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The indexing and slicing syntax for arrays with more than one axis uses a comma, such as `data[i,j]` for a 2-dimensional array. This differs from regular Python where separate brackets are needed for each sub-list, e.g. using `data[i][j]`. Accessing with multiple brackets will still work with NumPy arrays, but will be slower, as it makes an intermediate array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.arange(25).reshape(5,5)  # Two dimensional array\n",
    "print(y)\n",
    "print(y[2,3])     # One element specified with [Row, Column]\n",
    "print(y[1:4,1:4]) # A range of rows and columns (makes a new array)\n",
    "print(y[1,0:5])   # One entire row: all columns explicit\n",
    "print(y[-1])      # The entire last row: columns implicit\n",
    "print(y[-1,:])    # The entire last row, again \n",
    "print(y[:,2])     # One column (all rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuples of indices may be specified to make selections from an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3  4]\n",
      " [10 11 12 13 14]\n",
      " [20 21 22 23 24]]\n",
      "[ 0 12 24]\n"
     ]
    }
   ],
   "source": [
    "idx = (0,2,4)      # Row/column indices\n",
    "print(y[idx,:])    # Select specific rows, all columns\n",
    "print(y[idx, idx]) # Select row, column pairs : (0,0) (2,2) (4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy provides equivalent functions to the math module, which do the same thing, except to operate on all the elements of an array (and they also work on single numbers). As well as NumPy arrays the functions will accept regular Python lists or tuples as input, but an array is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.73205081 2.         2.23606798]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9.99999907, 2.71828183, 1.        ])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.sqrt(y))\n",
    "\n",
    "angles  = np.array([30.0, 60.0, 90.0, 135.0])\n",
    "radians = np.radians(angles)\n",
    "cosines = np.cos(radians)       # array([0.866, 0.50, 0.0, -0.707])\n",
    "np.log([10.0, 2.71828, 1.0]) # array([2.302585, 1.0, 0.0])\n",
    "np.exp([2.302585, 1.0, 0.0]) # array([10.0, 2.71828, 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various methods inbuilt into an array object, for example to calculate sums, extrema, mean and standard deviation. These may be applied to the whole array or only along a specific axis, via the axis argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[3,6],\n",
    "              [2,1],\n",
    "              [5,4]])\n",
    "\n",
    "x.min()          # 1                      - minimum value\n",
    "x.max()          # 6                      - maximum value\n",
    "x.max(axis=0)    # array([5,6])           - maximum value row\n",
    "x.sum()          # 21                     - summation of all\n",
    "x.sum(axis=1)    # array([9, 3, 9])       - add columns together  \n",
    "x.mean()         # 3.5                    - the mean value of all\n",
    "x.mean(axis=1)   # array([4.5, 1.5, 4.5]) - mean of each row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparisons between arrays generate Boolean arrays giving True or False for each elemental comparison. When doing this it is handy to know which value are true: nonzero() will get a tuple of non-zero index arrays than can be used to modify the elements that were true in the comparison. In a similar manner argsort() can be used to get the indices of an array in value order, which can then be used for sorting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 8, 9])\n",
    "y = np.array([5, 0, 3])\n",
    "z = x > y                # array([False, True, True])\n",
    "idx = z.nonzero()        # Indices where z is true (x value > y value)\n",
    "x[idx]                   # Values from x that were greater\n",
    "\n",
    "idx = y.argsort()   # ([1, 2, 0],) - index 1 is smallest, then 2 then 0\n",
    "y[idx]              # [[0, 3, 5]]  - in size order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many functions that relate to matrix operations and linear algebra, for example to calculate various products (inner, outer, cross), to transpose and find inverse matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(((1,1),(1,0)))\n",
    "y = np.array(((0,1),(1,1)))\n",
    "z = np.dot(x, y)               # array([[1, 2], [0, 1]]) - dot product\n",
    "                                # (matrix multiplication)\n",
    "\n",
    "x = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "y = x.transpose()              # array([[1, 4],[2, 5],[3, 6]])\n",
    "                               # (swapped rows with cols)\n",
    "\n",
    "x = numpy.array(((1,1),(1,0)))\n",
    "y = numpy.linalg.inv(x)        # array([[0., 1.], [1., -1.]]) - inverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy has a number of handy submodules such as fft for Fourier transforms and random for pseudo-random number generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import arrange, exp, fft, random\n",
    "\n",
    "l = 0.04\n",
    "w = 0.1\n",
    "t = arange(0.0, 100.0, 1.0)       # A range of time values\n",
    "x = exp(2j*pi*w*t) * exp(-l*t)    # Wave equation using complex numbers\n",
    "y = fft.fft(x)                    # Fast Fourier transform array of wave\n",
    "\n",
    "a = random.uniform(0.0, 5.0, 100) # Uniform sample 100 values in [0, 5)\n",
    "b = random.normal(0.0, 2.5, 100)  # Sample normal distribution\n",
    "                                  # Mean 0.0, standard deviation 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy installations include the matplotlib module, which provides the ability to make graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "from numpy import arange, random\n",
    "  \n",
    "x_vals = arange(1000)\n",
    "y_vals = random.poisson(5, 1000)\n",
    "pyplot.scatter(x_vals, y_vals, s=4, marker='*') # Make scatter plot\n",
    "pyplot.savefig(\"TestGraph.png\", dpi=72)         # Save graph as an image\n",
    "pyplot.show()                                   # Show on screen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciPy is an extensive library that builds upon the NumPy arrays and their functions. It provides specialized scientific functionality for areas including further linear algebra, optimization, integration, interpolation, signal processing, image processing and differential equations. Below a simple example is given, showing the ndimage sub-module which is useful for reading and writing image pixel data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "img_file = 'examples/my_image.png'\n",
    "pixmap = ndimage.imread(img_file)     # Read image data as array\n",
    "\n",
    "height, width, channels = pixmap.shape           \n",
    "red_channel   = pixmap[:,:,0]         # Color channels are last axis\n",
    "green_channel = pixmap[:,:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principle component analysis with NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function illustrates the use of the NumPy module and performs a principle component analysis: treating input data as vectors it finds the orthogonal directions in the data of maximal variance (the Eigenvectors of the covariance matrix). This is often used on high-dimensionality data to create simpler representations that still preserve the most important features. The function takes two arguments, the input data, which assumed to be equivalent to a list of vectors, and the number of principle components to extract. There is a small complication in this function as linalg.eig() outputs a matrix (p_comp_mat below) where each Eigenvector is a column, rather than a row. This orientation is useful for applying the matrix as a transformation, as we demonstrate below. Though, in the code it means the matrix is sorted and selected on its last axis (using [:,:n] etc.). Also, this is why the transpose, pcomps.T is used when extracting the two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random, dot, cov, linalg, concatenate, array \n",
    "from matplotlib import pyplot\n",
    "\n",
    "def get_principle_components(data, n=2):\n",
    "\n",
    "  data = array(data)               # Convert input to array\n",
    "\n",
    "  mean = data.mean(axis=0)         # Mean vector\n",
    "  centred_data = (data - mean).T   # Centre all vectors and transpose\n",
    "\n",
    "  covar = cov(centred_data)        # Get covariance matrix\n",
    "  evals, evecs = linalg.eig(covar) # Get Eigenvalues and Eigenvectors\n",
    "\n",
    "  indices = evals.argsort()[::-1]  # E. value indices by decreasing size\n",
    "\n",
    "  evecs = evecs[:,indices]    # Sort Eigenvecs according to Eigenvals\n",
    "\n",
    "  p_comp_mat = evecs[:,:n]    # Select required principle components\n",
    "\n",
    "  return p_comp_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is tested using some random 3D data. Here the random module from NumPy is used to create three clusters of vector points. Initially each has the same mean (0.0) and standard deviation (0.5), but the last two clusters are transposed by adding an offset vector. The clusters are concatenated together (along the long axis) to create the final test dataset with three ‘blobs’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (100, 3)                     # 100 points times 3 dimensions\n",
    "d1 = random.normal(0.0, 0.5, size) \n",
    "d2 = random.normal(0.0, 0.5, size) + array([4.0, 1.0, 2.0])\n",
    "d3 = random.normal(0.0, 0.5, size) + array([2.0, 0.0, -1.0])\n",
    "test_data = concatenate([d1, d2, d3], axis=0)\n",
    "\n",
    "pcomps = get_principle_components (test_data, n=2)   # Run PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The principle components are given back as a matrix, albeit in transposed form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc1, pc2 = pcomps.T   # Extract the two PC vectors from columns\n",
    "\n",
    "for x, y, z in (pc1, pc2):\n",
    "  x *= 10             # Scale value so it can be seen better on graph       \n",
    "  y *= 10\n",
    "  pyplot.plot((0, x), (0, y))  # Plot PC x, y as line from origin (0,0)\n",
    "\n",
    "x, y, x = test_data.T          # Extract x and y vals from transpose\n",
    "\n",
    "pyplot.scatter(x, y, s=20, c='#0080FF', marker='o', alpha=0.5)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The principle component matrix can be used to transform the test data. Here the first two principle components are used as new X and Y axis directions, illustrating that the transformation gives a better separated 2D view of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = dot(test_data, pcomps)\n",
    "\n",
    "x, y = transformed.T\n",
    "pyplot.scatter(x, y, s=20, c='#FF0000', marker='^')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics using SciPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciPy provides objects representing random variables for a variety of different statistical distributions. These are created by specifying the particular parameters for the distribution (e.g. mean and standard deviation for normal/Gaussian). Here a Poisson distribution is illustrated. A random variable object is created from which we access the probability mass function pmf(). This generates the probabilities of the input values, according to the probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm, poisson\n",
    "\n",
    "poisson_rand_var = poisson(2.0)       # Random variable object\n",
    "\n",
    "x_xals = arange(0, 10, 1)             # Value to plot probabilities for\n",
    "y_vals = poisson_rand_var.pmf(x_xals) # Get probabilities from distrib.\n",
    "\n",
    "pyplot.plot(x_xals, y_vals, color='black') # Make line plot \n",
    "pyplot.show()                              # Show on screen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next SciPy statistics example uses the random variable objects in a different way, to perform a tailed-test, as would be done to estimate a p-value: the probability of obtaining a value (or more extreme) from a given random distribution with stated parameters. The function performs the test on an input array of numbers for a normal distribution with given mean value, mv and standard deviation, std. There is an option to state if we want to do a one- or two-tailed test, i.e. consider only values on the same side of the mean or both sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_tail_test(values, mv, std, one_sided=True):\n",
    "  \n",
    "  norm_rv = norm(mv, std)     # Normal distrib. random variable object\n",
    "  \n",
    "  diffs = abs(values-mv)         # Calc differences from distrib. mean\n",
    "  result = norm_rv.cdf(mv-diffs) # Use cumulative density function\n",
    "  \n",
    "  if not one_sided:              # Two-tailed test\n",
    "    result *= 2                  # Distrib. is symmetric: double area\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function can be tested given some parameters and test values. In this case the values could represent the heights of male humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean    = 1.76\n",
    "std_dev = 0.075\n",
    "values  = array([1.8, 1.9, 2.0])\n",
    "\n",
    "result = normal_tail_test (values, mean, std_dev, one_sided=True)\n",
    "print('Normal one-tail', result)         # [0.297, 0.03097, 0.000687]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting DNA read quality scores from FASTQ files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the examples move on to illustrate handling high-throughput sequencing data. First we will work with FASTQ sequence read format and will handle this directly in standard Python, though in the next example an external module, pysam is used to deal with the file reading and etc.\n",
    "\n",
    "The objective for the following function is to read a stated number of quality scores from each sequence read of a FASTQ file and plot these as a graph. Naturally, the function takes the name of the file to read as an input argument, as well as a number of nucleotide positions to look at: this could be the whole read length or shorter. The operation of the function is fairly simple as it reads the lines of the file and collects quality scores. However, because the FASTQ format consists of four lines for each entry (annotation, sequence, annotation and qualities) the lines are read in groups of four using readline() and a while loop. There is a slight complication because the data can be shorter than our requested read length (some sequences terminate early). In such cases we extend the quality data with a padding string containing spaces that allows all quality data to be analysed in the same way (the padding spaces give zeros sores). It is notable that the quality scores in FASTQ files are stored as characters; one for each position in the DNA read sequence. These characters are converted to numeric values by the NumPy fromstring() function; each character is converted into its ASCII code number and the smallest value (32 for a space) is subtracted to get scores ranging from zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array, zeros, fromstring, int8, arange\n",
    "from matplotlib import pyplot\n",
    "\n",
    "def plot_fastq_qualities(fastq_file, read_len=100):\n",
    "    \n",
    "  file_obj = open(fastq_file)    # Open FASTQ file for reading\n",
    "\n",
    "  qual_scores = []               # Initial scores are empty list\n",
    "\n",
    "  line = file_obj.readline()     # Read first line (a header)\n",
    "  pad = ' ' * read_len           # Lots of spaces (to pad short data)\n",
    "\n",
    "  while line: # Continue looping while there are more lines\n",
    "\n",
    "    seq = file_obj.readline()    # Sequence line; not used \n",
    "    head = file_obj.readline()   # Second header line; not used \n",
    "    codes = file_obj.readline()  # Quality score code line; used\n",
    "    codes = codes.strip()        # Remove trailing newline char etc  \n",
    "\n",
    "    if len(codes) < read_len:    # If quality code string too short\n",
    "      codes += pad               # Extend with spaces\n",
    " \n",
    "    codes = codes[:read_len]     # Chop codes string to desired length\n",
    " \n",
    "    qvals = fromstring(codes, dtype=int8) # Convert string to numbers\n",
    "    qvals -= 32                  # Set lowest possible value to zero\n",
    "    \n",
    "    qual_scores.append(qvals)    # Add qual. scores for this seq to list\n",
    "    line = file_obj.readline()   # Read next header line\n",
    " \n",
    "  qual_scores = array(qual_scores)   # Convert final list to array\n",
    "  positions = arange(1, read_len+1)  # Make array of seq. positions\n",
    "\n",
    "  ave_scores = qual_scores.mean(axis=0)  # Find mean for each position\n",
    "  st_devs = qual_scores.std(axis=0)      # Standard dev. for each pos.\n",
    "\n",
    "  # Plot errorbars of stardard dev. height at score positions\n",
    "  pyplot.errorbar(positions, ave_scores, yerr=st_devs, color='red')\n",
    "\n",
    "  # Plot a line graph of scores on top\n",
    "  pyplot.plot(positions, ave_scores, color='black', linewidth=2)\n",
    "\n",
    "  pyplot.show()\n",
    "\n",
    "fastq_file = '/data/My_DNA_sample.fastq' # Test FASTQ file location\n",
    "plot_fastq_qualities(fastq_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
