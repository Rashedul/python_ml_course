{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LMB Python DNN Course Part 2\n",
    "\n",
    "### Importing modules\n",
    "\n",
    "The first commands in this notebook perform some initial set-up and module imports, as we did in Part 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import numpy as np  # Load the NumPy module, assign it the name \"np\" for convienence\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (16,8) # Set plot size\n",
    "\n",
    "from keras import models, layers, regularizers\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use the same function from Part 1 to plot the history of the DNN training, i.e. to report the loss and accuracy, for both training and test data, during the training iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(*histories):\n",
    "    cmap = cm.get_cmap('tab10')\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    plot_options = {'linewidth':2, 'alpha':0.5} # A dictionary of inputs for all charts\n",
    "    \n",
    "    for i, history in enumerate(histories):\n",
    "        hd = history.history\n",
    "        n = np.arange(len(hd['loss'])) + 1\n",
    "        plot_options['color'] = cmap(float(i % 10)/10)\n",
    "        \n",
    "        ax1.plot(n, hd['loss'], label='Train %d' % i,\n",
    "                 linestyle='--', **plot_options)\n",
    "        ax1.plot(n, hd['val_loss'], label='Test %d' % i,\n",
    "                 **plot_options)\n",
    "        ax1.set_title('Loss')\n",
    "        ax1.set_xlabel('Iteration')\n",
    "        \n",
    "        ax2.plot(n, hd['accuracy'], label='Train %d' % i,\n",
    "                 linestyle='--', **plot_options)\n",
    "        ax2.plot(n, hd['val_accuracy'], label='Test %d' % i,\n",
    "                 **plot_options)\n",
    "        ax2.set_title('Accuracy')\n",
    "        ax2.set_xlabel('Iteration')\n",
    "        \n",
    "    ax1.legend()    \n",
    "    ax2.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting ptotein transmembrane spans\n",
    "\n",
    "We now turn to our second example dataset which relates to biological sequences. The objective here will be to train a DNN to predict the location and extent of a protein's transmembrane span(s), if it has any.  \n",
    "\n",
    "For this we will use an existing, published dataset that is provided by the paper that goes with the [TMHMM](https://www.ncbi.nlm.nih.gov/pubmed/11152613) transmembrane prediction program. As the name suggests this method used a hidden markov model (HMM) to learn and perform predictions. We hope to do better using a DNN.\n",
    "\n",
    "In some sense, the TM spans of proteins are relatively easy features to predict from protein sequence alone because the TM span sits in a hydrophobic lipid bilayer and so the amino acids will tend to be substantially hydrophobic in nature. However, the situation is not so straightforward when charged residues are present in the TM span (can be a false negative), when aqueous globular domains have long internal hydrophobic helices (can be false positive), when multi-span TM proteins have small internal helicesm, and when the protein possesses a hydrophobic signalling peptide (e.g. to gain entry to the secretory system), which then is cleaved from the final protein.\n",
    "\n",
    "<img src=\"images_ML/tm_prot.png\" style=\"float:left;width:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to our DNNs will be amino acid sequences, albeit in an encoded form. And the output will be a prediction of 'membrane' or 'non-membrane' categories. Just like the original TMHMM program we will also further separate the non-membrane residues into an internal/cytoplasmic class and an external/lumenal/exoplamic class, and thus also give a prediction of the protein's membrane spanning topology.\n",
    "\n",
    "### Training data\n",
    "\n",
    "The training and test data will come from files that were provided when the TMHMM program was published. This has saved a lot of effort for using this as a tutorial dataset and we can trust that it is well curated. \n",
    "\n",
    "Because the training and test data are coming from bespoke formatted text files we first construct a function that can read the data into the NumPy arrays that we want to work with. The format of the file for each protein entry looks like the below example, i.e. with a protein amino acid sequence and corresponding codes to indicate whether a residue is inside `i`, membraneous `M`, or outside`o`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('set160.labels') as file_obj:\n",
    "    for i in range(4):\n",
    "        print(file_obj.readline().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function to read these data is a little fiddly, but a Python function that does this is provided below. \n",
    "A second training set was release with a later version of the TMHMM method; this has 243 proteins in it compared to the original 160. We will consider both in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable size input\n",
    "\n",
    "One of the innate problems when dealing with many types of biological sequence is that the lengths of the sequences can vary. This is an issue for neural networks that operate with a fixed size of inputs.\n",
    "\n",
    "The way that we will deal with this here is to consider a sliding window, which will be of a fixed size. In other words we will only be looking at part of the protein sequence at one time. For the purposes of predicting TM spans this approach will work as long as the window is wide enough. \n",
    "\n",
    "We will be predicting the inside `\"i\"`, outside `\"o\"` or membrane `\"M\"` category of the window's central residue(s), and so the extent of the window provides a sequence context either side of the middle. Naturally this context needs to be large enough to provide sufficient information for the prediction. For example if the window were only a few residues then this would not see the whole of a TM span and there would be no way to distinguish between small hydrophobic segments in non-membrane situations. We find that more than 50 residues works fairly well, as this is wide enough to cover a large TM span and a few residues either side; included some non-membrane residues is important for assessing the inside/outside topology; the topology flips either side of the TM span.\n",
    "\n",
    "When using a sliding window the edges of the protein sequence are potentially a problem; we need to extend the window over the edge of the protein sequence so the middle residue (in the window) can be one of the ofrst or last amino acids. We will fox this issue by extending the sequence with padding (`'-'` characters) so that the window cannot fall of the edges. Accompanying this we introduce a new data label (also `'-'`), which in effect is a class for empty, edge space. This presents little problem for the DNN and indeed having an edge context with its own special label can be helpful if the rules differ near the N- and C-terminii. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images_ML/TM_DNN_window.png\" style=\"float:left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be taking a short cut with this example as the DNNs input for non-membrane residues will be the non-membrane parts of the membrane proteins, rather than fully aqueous proteins. Choosing the later would be a better option in reality, although you have to be careful to have a degree of balance between positive (TM) and negative (no TM) training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation\n",
    "\n",
    "The practice of using a sliding sequence window means that each protein will be used many times as different input; i.e. with different offsets. We  could have looked at only sequence regions that are centred on each TM span. However, given that the number of TM protein examples is fairly small this would be somewhat limiting. Using a sliding window so that each sequence is used many times is effectively a technique called *data augmentation*; the same input is used many times with different offests or transformations (for image data the generally means scaling and rotation). Each different view of the data will present a different, albeit relates, context and effectively increases the amount of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading training and test data\n",
    "\n",
    "The below function reads the TM data file and converts the 1-letter codes for the amino acid sequence and the transmembrane category into Python integer numbers. This is done with a sliding window of specified size (via `win_size`) such that there will be several offset inputs fpr each protein. The `is_main` variable determines if the protein, with all its sliding-window sub-sequences, goes into the main training data or else the test data. The `test_frac` determines what proportion of the data is used as test. Note that this is only an approximate fraction as the function will not split a protein between test and training; overlapping sub-sequences are highly correlated and would introduce bias in the training set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random, seed\n",
    "\n",
    "CLASSES = 'ioM-'\n",
    "AA = 'ACDEFGHIKLMNPQRSTVWY-'\n",
    "AA_IDX = dict([(x,i) for i,x in enumerate(AA)])\n",
    "LABEL_IDX  = dict([(x,i) for i,x in enumerate(CLASSES)])\n",
    "NAA = len(AA)\n",
    "\n",
    "def load_tm_data(file_path, win_size=25, pred_width=1,\n",
    "                 stride=1, test_frac=0.1):\n",
    "\n",
    "  half_width = int(win_size/2)\n",
    "  pred_start = int(pred_width/2)\n",
    "  pred_end = pred_width-pred_start\n",
    "\n",
    "  train_data = []\n",
    "  test_data = []\n",
    "  train_labels = []\n",
    "  test_labels = []\n",
    "  \n",
    "  prot_data = {}\n",
    "    \n",
    "  pad = CLASSES[-1] * half_width \n",
    "  with open(file_path) as file_obj:\n",
    "    name = file_obj.readline().strip()[1:]\n",
    "    \n",
    "    while name:\n",
    "      is_main = len(test_data) > (test_frac * len(train_data))\n",
    "    \n",
    "      seq = file_obj.readline()[2:-1]\n",
    "      labels = file_obj.readline()[2:-1]\n",
    "      null = file_obj.readline()\n",
    "\n",
    "      n = len(seq)\n",
    "      prot_data[name] = (seq, labels, is_main)     \n",
    "\n",
    "      seq = pad + seq + pad\n",
    "      labels = pad + labels + pad\n",
    "        \n",
    "      for i in range(half_width, n+half_width, stride):\n",
    "        sub_seq = seq[i-half_width:i+half_width+1]\n",
    "        sub_seq = [AA_IDX[aa] for aa in sub_seq]\n",
    "        label = [labels[j] for j in range(i-pred_start,i+pred_end)] \n",
    "        label = [LABEL_IDX[j] for j in label]\n",
    "        \n",
    "        if is_main:\n",
    "          train_data.append(sub_seq)\n",
    "          train_labels.append(label)\n",
    "        else:\n",
    "          test_data.append(sub_seq)\n",
    "          test_labels.append(label)\n",
    "\n",
    "      name = file_obj.readline().strip()[1:]\n",
    " \n",
    "  train_data = np.array(train_data)\n",
    "  test_data = np.array(test_data)\n",
    "  train_labels = np.array(train_labels)\n",
    "  test_labels = np.array(test_labels)\n",
    "  \n",
    "  msg = 'Counts: training {:,}, test {:,} - Total proteins: {:,}'\n",
    "  print(msg.format(len(train_labels), len(test_labels), len(prot_data)))\n",
    "  \n",
    "  return (train_data, train_labels), (test_data, test_labels), prot_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properly separating the test data\n",
    "\n",
    "When we split the test data from the main training data we must be very careful that the datasets are truly separate. For example we may want to guarantee that the members of an homologous protein family are always in the same segment. With our sliding window approach we can investigate the effect of not separating the data properly by spreading different sliding windows for the same proteins across both test and training data. We could achieve this simply by shuffling the data loaded from file so that it no longer separated into different protein origins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training data\n",
    "\n",
    "We will start with a sliding window/region size of 20 residues. This will work to some degree, but we can easily investigate what happens with smaller or larger windows. With smaller regions we naturally get less sequence context, but training will be easier, as the input is smaller. Also, we will start with predicting only one, middle residue in our region and use the smaller of the TMHMM data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_size = 20\n",
    "train, test, prot_data = load_tm_data('set160.labels', win_size) # set243.labels or set160.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from the data reading function is then split into input and output sections for both the main training data and the test data.\n",
    "\n",
    "If we display the shapes of the resulting input data arrays we can see that the size of the last axis corresponds to the size of the sliding window. Within this the different amino acids (plus edge/gap) will be encoded, at least initially, as numbers 0-20. At this stage the outputs are simple, single categorical numbers 0-3 (for `'i'`, `'o'`, `'M'` or `'-'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = train\n",
    "test_data, test_labels = test\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(train_data[0])\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output, categorical labels will be converted into a binary (one-hot) encoding; a 1.0 in a particular column sets the category and other values are 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = to_categorical(train_labels, num_classes=4)\n",
    "test_labels = to_categorical(test_labels, num_classes=4)\n",
    "print(train_labels.shape)\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we perform a similar procedure to prepare the input data, i.e. converting it to categorical binary matrices and then flattening into vectors. Here we explicitly state the number of categorical classes (`NAA`) because there is a small chance that a protein won't contain all the different types of amino acid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = to_categorical(train_data, num_classes=NAA)\n",
    "test_data = to_categorical(test_data, num_classes=NAA)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because each data item is effectively a matrix, we flatten the last two axes into a single vector, as the DNN expects. The size of this vector is the inital matrix width time height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, a, b = train_data.shape\n",
    "m, a, b = test_data.shape\n",
    "\n",
    "train_data = train_data.reshape(n, a*b)\n",
    "test_data = test_data.reshape(m, a*b)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the loading of datasets easier later on. The above preparation for the input and output data is placed in a handy function called `prepare_data()`, which runs `load_tm_data()` internally and then performs the required categorical transformations etc. We can then run this function again when we switch to a different data file or change window size etc. \n",
    "\n",
    "This function also has some extra features which will be useful when trying more complex DNNs. Specifically there is: reshaping of the output labels in the section after `if train_labels.ndim > 2:`, for when we move from predicting just a single residues class to predicting classes for several residues; and an option to avoid converting the input data to a one-hot encoding (using `to_cat=False`) when we move to a learned encoding in the DNN itself.\n",
    "\n",
    "At the end of the function we send back the prepared input and outputs for the main training and test data, the sizes of the input and output vectors as we will need this when we construct the DNNs and finally sequence information relating to each protein; so that we can easily go back to the original sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_file, win_size=10, pred_width=1, stride=1, test_frac=0.1, to_cat=True):\n",
    "  \n",
    "  test, train, prot_data = load_tm_data(data_file, win_size, pred_width, stride, test_frac)\n",
    "  train_data, train_labels = test\n",
    "  test_data, test_labels = train\n",
    "\n",
    "  train_labels = to_categorical(train_labels, num_classes=len(CLASSES))\n",
    "  test_labels = to_categorical(test_labels, num_classes=len(CLASSES))\n",
    "\n",
    "  if train_labels.ndim > 2:\n",
    "    n, a, b = train_labels.shape\n",
    "    m, a, b = test_labels.shape # Last two axes are the same size\n",
    "    out_size = a * b\n",
    "\n",
    "    train_labels = train_labels.reshape(n, out_size) # Flatten last two axes\n",
    "    test_labels = test_labels.reshape(m, out_size)\n",
    "  else:\n",
    "    out_size = train_labels.shape[1]\n",
    "  \n",
    "  if to_cat:\n",
    "    train_data = to_categorical(train_data, num_classes=len(AA))\n",
    "    test_data = to_categorical(test_data, num_classes=len(AA))\n",
    "    n, a, b = train_data.shape\n",
    "    m, a, b = test_data.shape\n",
    "    in_size = a * b\n",
    "    train_data = train_data.reshape(n, in_size)\n",
    "    test_data = test_data.reshape(m, in_size)\n",
    "    \n",
    "  else:\n",
    "    in_size = train_data.shape[-1]\n",
    "  \n",
    "  return (train_data, train_labels), (test_data, test_labels), in_size, out_size, prot_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will re-load the training and test data to check this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_size = 40\n",
    "train, test, in_size, out_size, prot_data = prepare_data('set160.labels', win_size)\n",
    "(train_data, train_labels) = train\n",
    "(test_data, test_labels) = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial (fully connected) DNN architecture follows the same lines that were used at the end of the numeral recogntition example presented earlier. The neural network model is a sequential one with a number of `Dense` and `Dropout` layers, where the width of each layer diminishes toward the output. However, we use a `for` loop to avoid repetitive code.\n",
    "We set the last layer to (somewhat arbitrarily) be double the size of the output, so that it is definitely larger than the output vector, given the later will be changing in size as we tweak things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = [in_size]\n",
    "nn1 = models.Sequential()\n",
    "\n",
    "for width in [8*out_size, 2*out_size]:\n",
    "    nn1.add(layers.Dense(width, activation='relu', input_shape=input_shape)) \n",
    "    input_shape = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last layer is set to be the same size as the output vectors and then the DNN is finalised. Note that the loss function and paired last-layer activation function are appropriate for categorical selection where only one out[ut value can be true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1.add(layers.Dense(out_size, activation='softmax'))\n",
    "nn1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy',])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is initially for 10 iteractions (`epoch=`), updating the DNN with batches of 512 training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history1 = nn1.fit(train_data, train_labels, epochs=10, batch_size=512,\n",
    "                  validation_data=(test_data, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history of the loss and accuracy are plotted, as before, with our helper function `plot_training_history()`. We can see that predictor works  to an accuracy of about 75%. Note that this is the *per-residue* accuracy for the TM class assignment; even if some residues are incorrectly assigned the overall presence of a TM span will be more accurate. \n",
    "\n",
    "Unfortunately this network is subject to a notable amount of over-training; the loss and accuracy improve for the training data but not the test data. This is not unexpected, given that there are twenty different possible amino acids for each sequence position and so the number of total possible sequences is vast, and yet we have only sequenced a tiny fraction of this in the training data. In essence, long protein sequences are sparsely represented and thus potentially difficult to generalise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualise the predictive output categories some more helper functions are provided: given a DNN model and a sequence `get_seq_scores()` will get the predicted scores for each output category using a sliding window and `plot_seq_predict()` will display this as graphs. Note that the neural network model object `nn` is input and the `.predict()` function is used after each sub-sequence has been correctly converted to the same vector form used in the training. This function plots the scores for the four possibilities (inside, outside, membrane or sequence edge) as line graphs along the length of a test protein sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seq_scores(nn, seq, win_size, in_size, pred_size=1, categorical=True):\n",
    "  n = len(seq)\n",
    "  hw = int(win_size/2)\n",
    "  pad = '-' * hw\n",
    "  pseq = pad + seq + pad\n",
    "  scores = np.zeros((n, 4))  \n",
    "    \n",
    "  for k in range(n):\n",
    "    q = np.array([[AA_IDX[aa] for aa in pseq[k:k+hw+hw+1]]])\n",
    "    if categorical:\n",
    "      q = to_categorical(q, num_classes=NAA).reshape(1, in_size)\n",
    "    else:\n",
    "      q = q.reshape(1, in_size)\n",
    "    \n",
    "    out = nn.predict(q)\n",
    "    \n",
    "    if pred_size > 1:\n",
    "      out = out.reshape(pred_size, 4)[int(pred_size/2)]\n",
    "    \n",
    "    scores[k] = out\n",
    "\n",
    "  return scores\n",
    "\n",
    "def plot_seq_predict(nn, seq, win_size, in_size, pred_size=1, categorical=True):\n",
    "  scores = get_seq_scores(nn, seq, win_size, in_size, pred_size, categorical)\n",
    "\n",
    "  opts = {'alpha':0.4, 'linewidth':3}\n",
    "  i, o, m, x = scores.T\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  fig.set_size_inches(16.0, 3.0)  \n",
    "\n",
    "  ax.plot(i, color='#B0B040', label='Inside', **opts)\n",
    "  ax.plot(o, color='#4080FF', label='Outside', **opts)\n",
    "  ax.plot(m, color='#B00000', label='Membrane', **opts)\n",
    "  ax.plot(x, color='#808080', label='Edge', linestyle='--', **opts)\n",
    "  ax.set_xlabel('Sequence position')\n",
    "  ax.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequence predictions can now be run on any one-letter amino acid sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq1 = \"MYGKIIFVLLLSEIVSISASSTTGVAMHTSTSSSVTKSYISSQTNDTHKRDTYAATPRAHEVSEISVRTVYPPEEETGERVQLAHHFSEPEITLIIFGVMAGVIGTILLISYGIRRLIKKSPSDVKPLPSPDTDVPLSSVEIENPETSDQ\"\n",
    "seq2 = 'MWSTRSPNSTAWPLSLEPDPGMASASTTMHTTTIAEPDPGMSGWPDGRMETSTPTIMDIVVIAGVIAAVAIVLVSLLFVMLRYMYRHKGTYHTNEAKGTEFAESADAALQGDPALQDAGDSSRKEYFI'\n",
    "plot_seq_predict(nn1, seq1, win_size, in_size)\n",
    "plot_seq_predict(nn1, seq2, win_size, in_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can investigate what happens if we change the widths of the full-connected layers and the number of layers. As you night expect, smaller networks suffer less from overtraining but having a network that is too shallow and/or narrow will reduce test accuracy. Conversely larger networks tend to over-train, get slow and may show no benefit to test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=purple>Exercise 1: Tweaking layers</font>\n",
    "\n",
    "<font color=purple>See if it is possible to optimise the widths and number of `Dense` layers to improve the DNN test data loss and accuracy.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise code\n",
    "\n",
    "input_shape = [in_size]\n",
    "nn2 = models.Sequential()\n",
    "\n",
    "# Insert more layers here....\n",
    "    \n",
    "nn2.add(layers.Dense(out_size, activation='softmax'))\n",
    "nn2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy',])\n",
    "\n",
    "history2 = nn2.fit(train_data, train_labels, epochs=10, batch_size=512,\n",
    "                  validation_data=(test_data, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history1, history2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input indow size\n",
    "\n",
    "As well as the DNN parameters we can naturally also optimise the way in which the sequence is used as input.  For example here we train and test the network on input with a somewat larger window size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_size = 75\n",
    "train, test, in_size, out_size, prot_data = prepare_data('set160.labels', win_size)\n",
    "(train_data, train_labels) = train\n",
    "(test_data, test_labels) = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = [in_size]\n",
    "nn3 = models.Sequential()\n",
    "\n",
    "for width in [8*out_size, 2*out_size]:\n",
    "    nn3.add(layers.Dense(width, activation='relu', input_shape=input_shape)) \n",
    "    input_shape = []\n",
    "    \n",
    "nn3.add(layers.Dense(out_size, activation='softmax'))\n",
    "nn3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy',])\n",
    "\n",
    "history3 = nn3.fit(train_data, train_labels, epochs=10, batch_size=512,\n",
    "                   validation_data=(test_data, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the window size can give some improvement in accuracy, but larger input naturally makes the data more sparse and thus easier to overtrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history1, history3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq_predict(nn3, seq1, win_size, in_size)\n",
    "plot_seq_predict(nn3, seq2, win_size, in_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter searches\n",
    "\n",
    "Rather then manually searching for optimised parameters, these can naturally be sought in an automated manner with programming. A simple, though time consiming, approach here is an exhaustive seach that re-runs the DNN training many times and records the performance for a range (or grid) of paramaters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "win_sizes = range(50, 100, 10)\n",
    "for win_size in win_sizes:\n",
    "    train, test, in_size, out_size, prot_data = prepare_data('set160.labels', win_size)\n",
    "    (train_data, train_labels) = train\n",
    "    (test_data, test_labels) = test\n",
    "    input_shape = [in_size]\n",
    "    nn = models.Sequential()\n",
    "\n",
    "    for width in [8*out_size, 2*out_size]:\n",
    "      nn.add(layers.Dense(width, activation='relu', input_shape=input_shape)) \n",
    "      input_shape = []\n",
    "    \n",
    "    nn.add(layers.Dense(out_size, activation='softmax'))\n",
    "    nn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy',])\n",
    "\n",
    "    h = nn.fit(train_data, train_labels, epochs=3, batch_size=512,\n",
    "                validation_data=(test_data, test_labels))\n",
    "    accs.append(h.history['val_accuracy'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(win_sizes, accs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving predictions by predicting more\n",
    "\n",
    "When we inspect some of the TM category predictions for individual residue we can see that a common kind of error is a small segment (often only a single residue) that flips to the opposite inside/outside category compared to its neighbours. Naturally, this is not observed in the training class labels. Predicting the inside/outside class is more difficult that the membrane class, so mistakes are not surprising (generally this is base on something like the postive-inside rule near the TM span). This problem can be largely ameliorated by actually making more residue predictions. For example, rather than just predicting the i/o/M class for one central residue we can predict the class for several central residues. The DNN will learn that the categories are largely continuous and that transitions like 'iiooM' are never seen in the training data. In effect the adjacent residues give each other more context, albeit in the form of a predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_size = 60\n",
    "pred_width = 3\n",
    "train, test, in_size, out_size, prot_data = prepare_data('set160.labels', win_size, pred_width)\n",
    "(train_data, train_labels) = train\n",
    "(test_data, test_labels) = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when we train a network for an output where multiple classifications can be true at once we should swith to the `binary_crossentropy` loss function and pair this with the `sigmoid` activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = [in_size]\n",
    "nn4 = models.Sequential()\n",
    "\n",
    "for width in [8*out_size, 2*out_size]:\n",
    "    nn4.add(layers.Dense(width, activation='relu', input_shape=input_shape)) \n",
    "    input_shape = []\n",
    "    \n",
    "nn4.add(layers.Dense(out_size, activation='sigmoid'))\n",
    "nn4.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',])\n",
    "\n",
    "history4 = nn4.fit(train_data, train_labels, epochs=10, batch_size=512,\n",
    "                   validation_data=(test_data, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you will see that predicting for multiple adjascent residues works really well with both better loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history1, history4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising the score profiles along a couple of test sequences shows that the predictions are now smoother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq_predict(nn4, seq1, win_size, in_size, pred_width)\n",
    "plot_seq_predict(nn4, seq2, win_size, in_size, pred_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=purple>Exercise 2: Choosing the loss function</font>\n",
    "\n",
    "<font color=purple>Test what happens to the performance of the DNN if you keep using the `categorical_crossentropy` loss function and the corresponding `softmax` activation function for the last layer, as we did initially when only predicting for one residue.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = [in_size]\n",
    "nn3b = models.Sequential()\n",
    "\n",
    "for width in [8*out_size, 2*out_size]:\n",
    "    nn3b.add(layers.Dense(width, activation='relu', input_shape=input_shape)) \n",
    "    input_shape = []\n",
    "    \n",
    "# Fill in here\n",
    "\n",
    "history3b = nn3b.fit(train_data, train_labels, epochs=10, batch_size=512,\n",
    "                   validation_data=(test_data, test_labels))\n",
    "\n",
    "plot_training_history(history3, history3b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing over-training\n",
    "\n",
    "We can employ the same regularisation and droput strategies as we used with the numeral recognition in Part 1 to try to minimise the overtraining, i.e. where the tes and training loss/accuracy diverge; indicating that the DNN has become too specialised on the training data and less general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = [in_size]\n",
    "nn5 = models.Sequential()\n",
    "reg = regularizers.l2(0.0005)\n",
    "\n",
    "for width in [8*out_size, 2*out_size]:\n",
    "    #nn5.add(layers.Dense(width, activation='relu', input_shape=input_shape))\n",
    "    nn5.add(layers.Dense(width, activation='relu', input_shape=input_shape, kernel_regularizer=reg))\n",
    "    nn5.add(layers.Dropout(0.2))\n",
    "    input_shape = []\n",
    "    \n",
    "nn5.add(layers.Dense(out_size, activation='sigmoid'))\n",
    "nn5.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',])\n",
    "\n",
    "history5 = nn5.fit(train_data, train_labels, epochs=10, batch_size=512,\n",
    "                   validation_data=(test_data, test_labels))\n",
    "\n",
    "plot_training_history(history4, history5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = nn5.evaluate(train_data, train_labels)\n",
    "msg = 'Train Loss: {:.3f} Acc: {:.2f}%'.format(loss, 100.0*acc)\n",
    "print(msg)\n",
    "loss, acc = nn5.evaluate(test_data, test_labels)\n",
    "msg = 'Test Loss: {:.3f} Acc: {:.2f}%'.format(loss, 100.0*acc)\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of tweaking to this style of network doesn't give much improvement to the test accuracy. So next we move on to consider different types of network layer.\n",
    "\n",
    "Because we have a problem with the sparseness of amino acid sequences and generalisation to other sequences, we will introduce a layer that provides an alternative to the (one-hot) binary categorical matrix we initially used as input. This is an `Embedding` layer. In essence, this provides a lower dimensionality encoding of the amino acid sequences compared to the twenty-dimensional binary encoding. It works with the original index-encoded (0-20) form of the amino acid codes (hence `train_data_idx` etc.) and converts this to a low-dimensional vector; four in this case.\n",
    "\n",
    "A general encoding of the amino acids into a single, simple vector form doesn't work especially well. However, the Embedding layer will adopt a different encoding for each sequence position, which are learned (i.e. optimised) during the DNN training. The new layer is added before the Dense layers and its output must be flattened into a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_size = 60\n",
    "pred_width = 5\n",
    "train, test, in_size, out_size, prot_data = prepare_data('set160.labels', win_size, # or set243.labels\n",
    "                                              pred_width, to_cat=False)\n",
    "(train_data_idx, train_labels) = train\n",
    "(test_data_idx, test_labels) = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = regularizers.l2(0.0001)\n",
    "\n",
    "nn6 = models.Sequential()\n",
    "nn6.add(layers.Embedding(NAA, 4, input_length=in_size))\n",
    "nn6.add(layers.Flatten()) # Make a 1D vector\n",
    "\n",
    "for width in [out_size*8, out_size*2]:\n",
    "    nn6.add(layers.Dense(width, activation='relu', kernel_regularizer=reg))\n",
    "    nn6.add(layers.Dropout(0.2))\n",
    "    #nn6.add(layers.BatchNormalization())\n",
    "\n",
    "nn6.add(layers.Dense(out_size, activation='sigmoid'))\n",
    "nn6.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',])\n",
    "\n",
    "history6 = nn6.fit(train_data_idx, train_labels, epochs=10, batch_size=512,\n",
    "                   validation_data=(test_data_idx, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see by looking at the training history, this DNN performs better, especially with regards to the loss, and has even less over-trainaing. This can be attributed to the Embedding layer's encoding; likely the 4D positional encoding of the different amino acids represents axes of general predictive characteristics that extends better to unseen sequence combinations. An analogous, though general, predictive characteristic is residue hydrophobicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history5, history6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see how this improvement changes the graph of test sequence scores. The indide and outside categories are not so distinct, but the scores along the sequence are perhaps smoother:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq_predict(nn6, seq1, win_size, in_size, pred_width, categorical=False)\n",
    "plot_seq_predict(nn6, seq2, win_size, in_size, pred_width, categorical=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=purple>Exercise 3: Adding more data</font>\n",
    "\n",
    "<font color=purple>Compare the above network (`nn6`) when run on the two different input data files 'set160.labels' and 'set243.labels' Visualise both training histories.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining errors\n",
    "\n",
    "As should be common practice with training DNNs, we will investigate out data to see where the worst errors occur, to see if that gives us any clue about how to improve the training. Alternatively, we may find a problem with our training or test data, which might have mistakes... \n",
    "\n",
    "To help with this we first define a function `plot_seq_known` that simply plots the known categorical labels for a protein as a line graph that we can directly compare with the predictions, in a similar way to `plot_seq_predict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get worst input sub_seq\n",
    "def plot_seq_known(known, title):\n",
    "  opts = {'alpha':0.4, 'linewidth':3}\n",
    "  i, o, m, x = known.T\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  fig.set_size_inches(16.0, 3.0)  \n",
    "\n",
    "  ax.set_title(title)\n",
    "  ax.plot(i, color='#B0B040', label='Inside', **opts)\n",
    "  ax.plot(o, color='#4080FF', label='Outside', **opts)\n",
    "  ax.plot(m, color='#B00000', label='Membrane', **opts)\n",
    "  ax.plot(x, color='#808080', label='Edge', linestyle='--', **opts)\n",
    "  ax.set_xlabel('Sequence position')\n",
    "  ax.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we loop through the protein data that we have bee storing in `prot_data`, which is output when we loaded the data files. This contains a dictionary, keyed by protein name, to record the proteins, sequence, known categorical labels and whether it was in the main training set (or test set).\n",
    "\n",
    "We take proteins that are in the test set and compare the predicted scores from a DNN with the known labels (converted to a binary categorical encoding). The comparison involves simply calculating the distance between the output categorcal and known vector, then the iverall score for the protein is the average of this. Finally the proteins with the accompanying socres are sorted worst-first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nn = nn6\n",
    "seq_scores = []\n",
    "\n",
    "for prot_name in sorted(prot_data):\n",
    "    seq, labels, is_main = prot_data[prot_name]\n",
    "    \n",
    "    if not is_main:\n",
    "        scores = get_seq_scores(test_nn, seq, win_size, in_size, pred_width, categorical=False)\n",
    "        known = to_categorical(np.array([LABEL_IDX[l] for l in labels]), num_classes=len(CLASSES))\n",
    "        d2 = (scores - known)**2\n",
    "        dist = d2.sum(axis=1) ** 0.5\n",
    "        seq_score = dist.mean()\n",
    "        seq_scores.append((seq_score, prot_name, seq, known))\n",
    "\n",
    "seq_scores.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After collating the scores we can then inspect the test protein's prediction score and known categorical profiles (i.e. along the sequence) by using out helper functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "plot_seq_predict(test_nn, seqs[idx], win_size, in_size, pred_width, categorical=False)\n",
    "plot_seq_known(knowns[idx], prot_names[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we augment our DNN with another new type of layer: a `Conv1D`. As the name suggests, this is not a fully-connected layer. This is a convolutional layer; these feature heavily in the next course on images. The idea here is that the network tries to learn a number of fixed-width patterns (often called 'filters'). These patterns are short vectors that are applied to only a small patch of the input. Effectively this creates a local vocabulary of sub-structures within the training input. The notion is that the range of patterns is limited and so they become general, and match better on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_patterns = 11\n",
    "w_pattern = 11\n",
    "reg = regularizers.l2(0.0001)\n",
    "\n",
    "nn7 = models.Sequential()\n",
    "nn7.add(layers.Embedding(NAA, 4, input_length=in_size))\n",
    "\n",
    "nn7.add(layers.Conv1D(n_patterns, w_pattern, activation='relu',))\n",
    "nn7.add(layers.Flatten())\n",
    "\n",
    "for width in [out_size*8, out_size*2]:\n",
    "    nn7.add(layers.Dense(width, activation='relu', kernel_regularizer=reg))\n",
    "    nn7.add(layers.Dropout(0.2))\n",
    "\n",
    "nn7.add(layers.Dense(out_size, activation='sigmoid'))\n",
    "nn7.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history7 = nn7.fit(train_data_idx, train_labels, epochs=20, batch_size=512,\n",
    "                   validation_data=(test_data_idx, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training of this DNN takes more time than the others, but some improvement is visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history6, history7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pleasingly, the predictions are generally smoother for adjascent residues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq_predict(nn7, seq1, win_size, in_size, pred_width, categorical=False)\n",
    "plot_seq_predict(nn7, seq2, win_size, in_size, pred_width, categorical=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=purple>Exercise 4: Getting above 95%</font>\n",
    "\n",
    "<font color=purple>Try to tweak the above DNN to get a test accuracy of >95% within 20 iterations. You can adjust the window sizes, output prediction with and internal DNN parameters.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Exercise code\n",
    "\n",
    "win_size = # Fill in here\n",
    "pred_width = #  Fill in here\n",
    "\n",
    "train, test, in_size, out_size, prot_data = prepare_data('set243.labels', win_size,\n",
    "                                              pred_width, to_cat=False)\n",
    "(train_data_idx, train_labels) = train\n",
    "(test_data_idx, test_labels) = test\n",
    "\n",
    "n_patterns = 11 # Maybe adjust \n",
    "w_pattern = 11 # Maybe adjust \n",
    "reg = regularizers.l2(0.0001) # Maybe adjust \n",
    "\n",
    "nn8 = models.Sequential()\n",
    "nn8.add(layers.Embedding(NAA, 4, input_length=in_size)) # Maybe adjust from 4D\n",
    "\n",
    "nn8.add(layers.Conv1D(n_patterns, w_pattern, activation='relu',))\n",
    "nn8.add(layers.Flatten())\n",
    "\n",
    "for width in [out_size*8, out_size*2]: # Maybe adjust \n",
    "    nn8.add(layers.Dense(width, activation='relu', kernel_regularizer=reg))\n",
    "    nn8.add(layers.Dropout(0.2))\n",
    "\n",
    "nn8.add(layers.Dense(out_size, activation='sigmoid'))\n",
    "nn8.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',])\n",
    "\n",
    "plot_training_history(history7, history8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going further...\n",
    "\n",
    "That's the end of this course. However, there are a few ways that we might like to take this prediction in the future. Firstly, we can make much better use of fully aqueous proteins (i.e. no TM span), which could be used for fully 'inside' or fully 'ouside' sequence examples and serve as negative training examples. Secondly, we could expand the number of protein sequenxces by using close homologues, though here we would have to carefully balance the sizes of the protein families, and make sure they dont spread across the test/train split. Thirdly, we could predict leading signal peptides (e.g. for entry into the secretory system) and any cleavage site at the same time, which would hopfully remove a source of false positive errors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
